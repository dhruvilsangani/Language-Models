{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class biLM(nn.Module):\n",
    "    '''\n",
    "    initialize with\n",
    "    embedding: pre-trained embedding layer\n",
    "    hidden_size: size of hidden_states of biLM\n",
    "    n_layers: number of layers\n",
    "    dropout: dropout\n",
    "    '''\n",
    "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
    "        super(biLM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = embedding\n",
    "        self.forwardLSTM = nn.LSTM(hidden_size, \n",
    "                                         hidden_size, \n",
    "                                         n_layers, \n",
    "                                         dropout=(0 if n_layers == 1 else dropout))\n",
    "        self.backwardLSTM = nn.LSTM(hidden_size, \n",
    "                                         hidden_size, \n",
    "                                         n_layers, \n",
    "                                         dropout=(0 if n_layers == 1 else dropout))\n",
    "        \n",
    "    def forward(self, input_seq, input_lengths, initial_states=None):\n",
    "        '''\n",
    "        input_seq: size=(MAX_LEN, batch_size)\n",
    "        input_lengths: contains length of each sentence\n",
    "        initial_states: tuple of initial hidden_state of LSTM, initial cell state of LSTM\n",
    "        '''\n",
    "        embedded = self.embedding(input_seq)\n",
    "        MAX_LEN = embedded.size()[0]\n",
    "        # embedded: size=(MAX_LEN, batch_size, hidden_size)\n",
    "        outputs = torch.zeros(MAX_LEN, batch_size, 2, hidden_size)\n",
    "        hidden_states = torch.zeros(self.n_layers * 2, MAX_LEN, batch_size, hidden_size)\n",
    "        batch_size = embedded.size()[1]\n",
    "        \n",
    "        for batch_n in batch_size:\n",
    "            sentence = input_seq[:,batch_n, :]\n",
    "            length = input_lengths[batch_n]\n",
    "            \n",
    "            if initial_states:\n",
    "                hidden_forward_state, cell_forward_state = initial_states\n",
    "                hidden_backward_state, cell_backward_state = initial_states\n",
    "            else:\n",
    "                hidden_forward_state, cell_forward_state = None, None\n",
    "                hidden_backward_state, cell_backward_state = None, None\n",
    "                \n",
    "            for t in range(length):\n",
    "                output, (hidden_forward_state, cell_forward_state) = forwardLSTM(sentence[t], (hidden_forward_state, cell_forward_state))\n",
    "                outputs[t, batch_n, 0, :] = output[0, 0, :]\n",
    "                hidden_state[:n_layers, batch_n, t, :] = hidden_forward_state[:, 0, :]\n",
    "                \n",
    "            for t in range(length):\n",
    "                output, (hidden_backward_state, cell_backward_state) = backwardLSTM(sentence[length - t - 1], (hidden_backward_state, cell_backward_state))\n",
    "                outputs[length - t - 1, batch_n, 1, :] = output[0, 0, :]\n",
    "                hidden_state[n_layers:, batch_n, length - t - 1, :] = hidden_backward_state[:, 0, :]\n",
    "                \n",
    "        return outputs, hidden_states, embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class ELMo(nn.Module):\n",
    "    '''\n",
    "    initialize with\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0, l2_coef=None, do_layer_norm=False):\n",
    "        super(ELMo, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.l2_coef = l2_coef\n",
    "        self.use_top_only = use_top_only\n",
    "        self.do_layer_norm = do_layer_norm\n",
    "        self.biLM = biLM(hidden_size, embedding, n_layers, dropout)\n",
    "        self.W = torch.zeros(2*n_layers+1, requires_grad=True)\n",
    "        self.W = F.softmax(self.W + 1/(2*n_layers + 1))\n",
    "        self.gamma = torch.ones(1, requires_grad=True)\n",
    "        \n",
    "    def do_norm(layer, mask):\n",
    "        masked_layer = layer * mask\n",
    "        N = torch.sum(mask) * self.hidden_size\n",
    "        mean = torch.sum(masked_layer)/N\n",
    "        variance = torch.sum(((masked_layer - mean) * mask) ** 2) / N\n",
    "        \n",
    "        return F.batch_norm(layer, mean, variance)\n",
    "    \n",
    "    def forward(self, input_seq, input_lengths, mask, initial_states=None):\n",
    "        bilm_outputs, hidden_states, embedded = biLM(input_seq, input_lengths, initial_states)\n",
    "        concat_hidden_with_embedding = torch.cat(embedded.unsqueeze(0), hidden_states)\n",
    "        ELMo_embedding = gamma.item()\n",
    "        for i in range(2*n_layers + 1):\n",
    "            w = W[i]\n",
    "            layer = concat_hidden_with_embedding[i]\n",
    "            if do_layer_norm:\n",
    "                layer = do_norm(layer, mask)\n",
    "            ELMo_embedding = ELMo_embedding + w * layer\n",
    "        return ELMo_embedding, bilm_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
